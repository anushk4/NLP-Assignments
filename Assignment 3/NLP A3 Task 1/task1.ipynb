{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkQS2pOLWEt6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(self, specials=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]):\n",
        "        self.specials = specials\n",
        "        self.token2idx = {}\n",
        "        self.idx2token = []\n",
        "        for sp in specials:\n",
        "            self.add_token(sp)\n",
        "        self.pad_token = \"<PAD>\"\n",
        "        self.unk_token = \"<UNK>\"\n",
        "        self.bos_token = \"<BOS>\"\n",
        "        self.eos_token = \"<EOS>\"\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token not in self.token2idx:\n",
        "            self.token2idx[token] = len(self.idx2token)\n",
        "            self.idx2token.append(token)\n",
        "\n",
        "    def build_vocab(self, text_lines):\n",
        "        \"\"\"\n",
        "        Build the vocab by lowercasing and regex-based tokenization.\n",
        "        \"\"\"\n",
        "        for line in text_lines:\n",
        "            line = line.lower()  # Convert to lowercase\n",
        "            # Extract \"word\" tokens OR any single non-whitespace char (e.g. punctuation)\n",
        "            tokens = re.findall(r\"\\w+|[^\\w\\s]\", line)\n",
        "            for t in tokens:\n",
        "                self.add_token(t)\n",
        "\n",
        "    def encode(self, text_line, add_bos_eos=False):\n",
        "        \"\"\"\n",
        "        Convert a text line into token IDs using the same regex approach.\n",
        "        \"\"\"\n",
        "        text_line = text_line.lower()\n",
        "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text_line)\n",
        "        ids = []\n",
        "        if add_bos_eos:\n",
        "            ids.append(self.token2idx[self.bos_token])\n",
        "        for t in tokens:\n",
        "            if t in self.token2idx:\n",
        "                ids.append(self.token2idx[t])\n",
        "            else:\n",
        "                ids.append(self.token2idx[self.unk_token])\n",
        "        if add_bos_eos:\n",
        "            ids.append(self.token2idx[self.eos_token])\n",
        "        return ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"Convert a list of token IDs back into a string.\"\"\"\n",
        "        tokens = []\n",
        "        for i in token_ids:\n",
        "            tokens.append(self.idx2token[i])\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing, vocab_size, ignore_index=None):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, log_probs, target):\n",
        "        \"\"\"\n",
        "        log_probs: (batch_size*seq_len, vocab_size) [already passed through log_softmax]\n",
        "        target:    (batch_size*seq_len)\n",
        "        \"\"\"\n",
        "        # Confidence for the true class:\n",
        "        confidence = 1.0 - self.smoothing\n",
        "        # Amount to distribute over all other classes:\n",
        "        smoothing_value = self.smoothing / (self.vocab_size - 1)\n",
        "\n",
        "        # Create full one-hot distribution with smoothing\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.full_like(log_probs, smoothing_value)\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), confidence)\n",
        "\n",
        "            # If ignore_index is set, zero out those entries\n",
        "            if self.ignore_index is not None:\n",
        "                ignore_mask = (target == self.ignore_index).unsqueeze(1)\n",
        "                true_dist.masked_fill_(ignore_mask, 0.0)\n",
        "\n",
        "        loss = -torch.sum(true_dist * log_probs, dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "def read_lines_from_file(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    # Remove empty lines and strip\n",
        "    lines = [l.strip() for l in lines if l.strip() != \"\"]\n",
        "    return lines\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps):\n",
        "    \"\"\"\n",
        "    Cosine decay to 0, with a linear warm-up at the start.\n",
        "    \"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        # Cosine decay from 1 -> 0\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def prepare_dataset(filepath, tokenizer, seq_len=32):\n",
        "    lines = read_lines_from_file(filepath)\n",
        "    # Convert each line into token IDs\n",
        "    token_sequences = []\n",
        "    for line in lines:\n",
        "        token_ids = tokenizer.encode(line)\n",
        "        if len(token_ids) > 0:\n",
        "            token_sequences.append(token_ids)\n",
        "\n",
        "    # Now we create training examples of length seq_len (for x)\n",
        "    # and the next token as target (shifted by 1).\n",
        "    all_input_ids = []\n",
        "    all_target_ids = []\n",
        "\n",
        "    # Weâ€™ll just concatenate everything for language modeling.\n",
        "    # Alternatively, you can handle each line individually.\n",
        "    concat_ids = []\n",
        "    for seq in token_sequences:\n",
        "        concat_ids.extend(seq + [tokenizer.token2idx[tokenizer.eos_token]])  # end with <EOS>\n",
        "\n",
        "    # Slide over the concatenated IDs by seq_len\n",
        "    for i in range(0, len(concat_ids) - seq_len):\n",
        "        x = concat_ids[i : i + seq_len]\n",
        "        y = concat_ids[i + 1 : i + seq_len + 1]\n",
        "        if len(x) == seq_len and len(y) == seq_len:\n",
        "            all_input_ids.append(x)\n",
        "            all_target_ids.append(y)\n",
        "\n",
        "    return torch.LongTensor(all_input_ids), torch.LongTensor(all_target_ids)\n",
        "\n",
        "def generate_causal_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Returns a (seq_len, seq_len) mask where\n",
        "    positions (i, j) are 0 if j <= i, and -inf if j > i.\n",
        "    This ensures each token can only attend to itself\n",
        "    and previous tokens, not future ones.\n",
        "    \"\"\"\n",
        "    # shape: (seq_len, seq_len)\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "    return mask\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        # Add positional encoding up to seq_len\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    q, k, v: (batch_size, n_heads, seq_len, d_head)\n",
        "    mask: (1, 1, seq_len, seq_len) or (batch_size, 1, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores + mask  # add -inf where mask is True\n",
        "    attn = torch.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn, v)  # (batch_size, n_heads, seq_len, d_head)\n",
        "    return output, attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, d_model)\n",
        "        mask shape: (seq_len, seq_len) or broadcastable\n",
        "        \"\"\"\n",
        "        bsz, seq_len, _ = x.size()\n",
        "        q = self.w_q(x)  # (bsz, seq_len, d_model)\n",
        "        k = self.w_k(x)\n",
        "        v = self.w_v(x)\n",
        "\n",
        "        # Reshape into (bsz, num_heads, seq_len, d_head)\n",
        "        q = q.reshape(bsz, seq_len, self.num_heads, self.d_head).transpose(1,2)\n",
        "        k = k.reshape(bsz, seq_len, self.num_heads, self.d_head).transpose(1,2)\n",
        "        v = v.reshape(bsz, seq_len, self.num_heads, self.d_head).transpose(1,2)\n",
        "\n",
        "        # Expand mask if necessary\n",
        "        if mask is not None:\n",
        "            # (seq_len, seq_len) => (bsz, num_heads, seq_len, seq_len)\n",
        "            mask = mask.unsqueeze(0).unsqueeze(1)  # shape: (1, 1, seq_len, seq_len)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output, attn = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "        # attn_output: (bsz, n_heads, seq_len, d_head)\n",
        "\n",
        "        # Combine heads\n",
        "        attn_output = attn_output.transpose(1,2).contiguous().reshape(bsz, seq_len, self.d_model)\n",
        "        # Final linear\n",
        "        output = self.w_o(attn_output)  # (bsz, seq_len, d_model)\n",
        "        output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention\n",
        "        attn_out = self.attn(x, mask=mask)\n",
        "        x = x + attn_out  # Residual\n",
        "        x = self.ln1(x)\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + ff_out  # Residual\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=4, d_ff=1024, num_layers=4, dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Weight initialization (optional but recommended)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len)\n",
        "        Returns: logits of shape (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        bsz, seq_len = x.size()\n",
        "        # Create causal mask\n",
        "        mask = generate_causal_mask(seq_len).to(x.device)  # (seq_len, seq_len)\n",
        "\n",
        "        # Embeddings\n",
        "        x = self.token_emb(x) * math.sqrt(self.d_model)  # (bsz, seq_len, d_model)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through each Transformer block\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "\n",
        "        # Final layer norm, then project to vocab\n",
        "        x = self.ln(x)\n",
        "        logits = self.fc_out(x)  # (bsz, seq_len, vocab_size)\n",
        "        return logits\n",
        "\n",
        "def calculate_loss_and_perplexity(logits, targets, pad_idx=0):\n",
        "    \"\"\"\n",
        "    logits: (batch_size, seq_len, vocab_size)\n",
        "    targets: (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    # Flatten logits to (batch_size*seq_len, vocab_size)\n",
        "    vocab_size = logits.size(-1)\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    # We ignore pad tokens in the loss\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "    loss = loss_fn(logits, targets)\n",
        "\n",
        "    # Perplexity\n",
        "    # Only compute perplexity on non-PAD\n",
        "    mask = (targets != pad_idx)\n",
        "    n_tokens = mask.sum().item()\n",
        "    # cross_entropy = total_loss / n_tokens\n",
        "    # But we can just re-implement the cross-entropy on masked positions\n",
        "    with torch.no_grad():\n",
        "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
        "        log_probs = log_probs[mask]\n",
        "        targets_masked = targets[mask]\n",
        "        negative_log_likelihood = -log_probs[range(len(targets_masked)), targets_masked]\n",
        "        avg_nll = negative_log_likelihood.mean().item()\n",
        "        ppl = math.exp(avg_nll)\n",
        "\n",
        "    return loss, ppl\n",
        "\n",
        "def train_model(train_x, train_y, dev_x, dev_y, model, tokenizer,\n",
        "                epochs=10, batch_size=32, lr=1e-4, pad_idx=0,\n",
        "                smoothing=0.1, warmup_ratio=0.1, weight_decay=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # 1) Adam with weight decay\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # 2) Set up scheduler\n",
        "    total_steps = epochs * (train_x.size(0) // batch_size)\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    # 3) Label smoothing criterion\n",
        "    vocab_size = len(tokenizer)\n",
        "    ls_criterion = LabelSmoothingLoss(smoothing, vocab_size, ignore_index=pad_idx)\n",
        "\n",
        "    train_losses, dev_losses = [], []\n",
        "    train_ppls, dev_ppls = [], []\n",
        "\n",
        "    def get_batches(x_data, y_data, batch_size):\n",
        "        for i in range(0, x_data.size(0), batch_size):\n",
        "            yield x_data[i:i+batch_size], y_data[i:i+batch_size]\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_ppl = 0.0\n",
        "        count = 0\n",
        "\n",
        "        # Shuffle training data each epoch\n",
        "        idx = torch.randperm(train_x.size(0))\n",
        "        train_x = train_x[idx]\n",
        "        train_y = train_y[idx]\n",
        "\n",
        "        for bx, by in get_batches(train_x, train_y, batch_size):\n",
        "            bx, by = bx.to(device), by.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(bx)  # (B, seq, vocab)\n",
        "            # We'll flatten to compute label-smoothed cross-entropy\n",
        "            log_probs = F.log_softmax(logits.view(-1, vocab_size), dim=-1)\n",
        "            loss = ls_criterion(log_probs, by.view(-1))\n",
        "\n",
        "            # Backprop\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # step the LR scheduler per iteration\n",
        "            global_step += 1\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "            # (Optional) compute perplexity from this batch for logging\n",
        "            with torch.no_grad():\n",
        "                # gather the correct tokens' log-probs (ignoring pad)\n",
        "                mask = (by.view(-1) != pad_idx)\n",
        "                masked_log_probs = log_probs[mask, :]\n",
        "                masked_targets = by.view(-1)[mask]\n",
        "                # negative log-likelihood\n",
        "                nll = -masked_log_probs[range(masked_log_probs.size(0)), masked_targets].mean().item()\n",
        "                epoch_ppl += math.exp(nll)\n",
        "\n",
        "        train_losses.append(epoch_loss / count)\n",
        "        train_ppls.append(epoch_ppl / count)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        dev_loss = 0.0\n",
        "        dev_ppl_sum = 0.0\n",
        "        dev_count = 0\n",
        "        with torch.no_grad():\n",
        "            for bx, by in get_batches(dev_x, dev_y, batch_size):\n",
        "                bx, by = bx.to(device), by.to(device)\n",
        "                logits = model(bx)\n",
        "                log_probs = F.log_softmax(logits.view(-1, vocab_size), dim=-1)\n",
        "                loss = ls_criterion(log_probs, by.view(-1))\n",
        "                dev_loss += loss.item()\n",
        "\n",
        "                # perplexity\n",
        "                mask = (by.view(-1) != pad_idx)\n",
        "                masked_log_probs = log_probs[mask, :]\n",
        "                masked_targets = by.view(-1)[mask]\n",
        "                nll = -masked_log_probs[range(masked_log_probs.size(0)), masked_targets].mean().item()\n",
        "                dev_ppl_sum += math.exp(nll)\n",
        "\n",
        "                dev_count += 1\n",
        "\n",
        "        dev_loss /= dev_count\n",
        "        dev_ppl = dev_ppl_sum / dev_count\n",
        "        dev_losses.append(dev_loss)\n",
        "        dev_ppls.append(dev_ppl)\n",
        "\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_losses[-1]:.4f}, train_ppl={train_ppls[-1]:.2f}, \"\n",
        "              f\"dev_loss={dev_losses[-1]:.4f}, dev_ppl={dev_ppls[-1]:.2f}\")\n",
        "\n",
        "        # Simple generation test\n",
        "        sample_context = \"First Citizen :\"\n",
        "        generated_text = generate_text(model, sample_context, tokenizer, max_new_tokens=20)\n",
        "        print(\"Sample generation:\", generated_text)\n",
        "\n",
        "    return train_losses, dev_losses, train_ppls, dev_ppls\n",
        "\n",
        "\n",
        "def generate_text(model, context_text, tokenizer, max_new_tokens=20, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Encode the context\n",
        "    context_ids = tokenizer.encode(context_text)\n",
        "    context = torch.LongTensor(context_ids).unsqueeze(0).to(device)  # shape (1, len_context)\n",
        "\n",
        "    # Generate\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            logits = model(context)  # (1, seq_len, vocab_size)\n",
        "            logits = logits[:, -1, :]  # take the last position\n",
        "            # Optionally apply temperature\n",
        "            logits = logits / temperature\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "        # Append next token\n",
        "        context = torch.cat([context, next_token_id], dim=1)\n",
        "\n",
        "    # Convert IDs to text\n",
        "    generated_ids = context[0].cpu().numpy().tolist()\n",
        "    return tokenizer.decode(generated_ids)\n",
        "\n",
        "def main():\n",
        "    # === 1) Read data ===\n",
        "    train_file = \"/kaggle/input/shakespear/shakespear_train.txt\"\n",
        "    dev_file = \"/kaggle/input/shakespear/shakespear_dev.txt\"\n",
        "    test_file = \"test.txt\"  # provided at demo\n",
        "\n",
        "    train_lines = read_lines_from_file(train_file)\n",
        "    dev_lines = read_lines_from_file(dev_file)\n",
        "\n",
        "    # === 2) Build tokenizer ===\n",
        "    tokenizer = CustomTokenizer()\n",
        "    tokenizer.build_vocab(train_lines + dev_lines)\n",
        "    vocab_size = len(tokenizer)\n",
        "    print(f\"Vocab size = {vocab_size}\")\n",
        "\n",
        "    # === 3) Prepare the datasets ===\n",
        "    train_x, train_y = prepare_dataset(train_file, tokenizer, seq_len=32)\n",
        "    dev_x, dev_y = prepare_dataset(dev_file, tokenizer, seq_len=32)\n",
        "\n",
        "    print(\"train_x shape:\", train_x.shape, \"train_y shape:\", train_y.shape)\n",
        "    print(\"dev_x shape:\", dev_x.shape, \"dev_y shape:\", dev_y.shape)\n",
        "\n",
        "    # === 4) Create the model ===\n",
        "    d_model = 128\n",
        "    num_heads = 8\n",
        "    d_ff = 512\n",
        "    num_layers = 2\n",
        "    dropout = 0.2\n",
        "    pad_idx = tokenizer.token2idx[tokenizer.pad_token]\n",
        "    model = TransformerLanguageModel(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        d_ff=d_ff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        pad_idx=pad_idx\n",
        "    )\n",
        "\n",
        "    # === 5) Train the model ===\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    lr = 1e-3  # you can start a bit higher since we have warm-up\n",
        "    train_losses, dev_losses, train_ppls, dev_ppls = train_model(\n",
        "        train_x, train_y, dev_x, dev_y,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        lr=lr,\n",
        "        pad_idx=pad_idx,\n",
        "        smoothing=0.1,         # label smoothing\n",
        "        warmup_ratio=0.1,      # 10% of total steps are warm-up\n",
        "        weight_decay=1e-4      # L2 regularization\n",
        "    )\n",
        "\n",
        "    # === 6) Plot training & validation losses and perplexities ===\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(dev_losses, label=\"Dev Loss\")\n",
        "    plt.title(\"Loss vs. Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(train_ppls, label=\"Train Perplexity\")\n",
        "    plt.plot(dev_ppls, label=\"Dev Perplexity\")\n",
        "    plt.title(\"Perplexity vs. Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # === 7) Save the model ===\n",
        "    model_save_path = \"transformer_shakespeare.pt\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # === 8) Evaluate on the test set (demo) ===\n",
        "    # We can load the model from disk if we want fresh:\n",
        "    # model2 = TransformerLanguageModel(vocab_size, d_model, num_heads, d_ff, num_layers, dropout, pad_idx)\n",
        "    # model2.load_state_dict(torch.load(model_save_path))\n",
        "    # model2.eval()\n",
        "\n",
        "    # Let's do test perplexity (assuming test data also prepared):\n",
        "    if os.path.exists(test_file):\n",
        "        test_x, test_y = prepare_dataset(test_file, tokenizer, seq_len=32)\n",
        "        if test_x.size(0) > 0:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            test_loss, test_ppl = 0.0, 0.0\n",
        "            test_count = 0\n",
        "            for i in range(0, test_x.size(0), batch_size):\n",
        "                bx = test_x[i:i+batch_size].to(device)\n",
        "                by = test_y[i:i+batch_size].to(device)\n",
        "                with torch.no_grad():\n",
        "                    logits = model(bx)\n",
        "                    loss, ppl = calculate_loss_and_perplexity(logits, by, pad_idx=pad_idx)\n",
        "                test_loss += loss.item()\n",
        "                test_ppl += ppl\n",
        "                test_count += 1\n",
        "            test_loss /= test_count\n",
        "            test_ppl /= test_count\n",
        "            print(f\"Test Loss: {test_loss:.4f}, Test PPL: {test_ppl:.2f}\")\n",
        "\n",
        "        # Also do generation from each line in test.txt\n",
        "        test_lines = read_lines_from_file(test_file)\n",
        "        for line in test_lines:\n",
        "            prompt = line.strip()\n",
        "            gen_text = generate_text(model, prompt, tokenizer, max_new_tokens=20)\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            print(f\"Generated: {gen_text}\")\n",
        "            print(\"----\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_and_perplexity(logits, targets, criterion, pad_idx=0):\n",
        "    \"\"\"\n",
        "    logits:  (batch_size, seq_len, vocab_size)\n",
        "    targets: (batch_size, seq_len)\n",
        "    criterion: a loss function (e.g., LabelSmoothingLoss)\n",
        "\n",
        "    Returns: (loss, ppl) for this batch\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, vocab_size = logits.size()\n",
        "    log_probs = F.log_softmax(logits.view(-1, vocab_size), dim=-1)\n",
        "    loss = criterion(log_probs, targets.view(-1))\n",
        "\n",
        "    # ignoring padding tokens\n",
        "    mask = (targets.view(-1) != pad_idx)\n",
        "    # Gather the correct token log-probs\n",
        "    masked_log_probs = log_probs[mask, :]\n",
        "    masked_targets = targets.view(-1)[mask]\n",
        "\n",
        "    nll = -masked_log_probs[range(masked_log_probs.size(0)), masked_targets].mean().item()\n",
        "    ppl = math.exp(nll)\n",
        "    return loss, ppl\n",
        "\n",
        "def inference(model_path, test_file, tokenizer, pad_idx=0, max_new_tokens=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    1) Loads the pretrained model from 'model_path'\n",
        "    2) Reads and preprocesses 'test_file'\n",
        "    3) Generates 'max_new_tokens' tokens for each line in test_file\n",
        "    4) Calculates perplexity on the test set\n",
        "    \"\"\"\n",
        "    # 1) Loading the pretrained model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # re-instantiating the same model architecture used during training:\n",
        "    d_model = 256\n",
        "    num_heads = 16\n",
        "    d_ff = 1024\n",
        "    num_layers = 4\n",
        "    dropout = 0.2\n",
        "\n",
        "    model = TransformerLanguageModel(\n",
        "        vocab_size=len(tokenizer),\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        d_ff=d_ff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        pad_idx=pad_idx\n",
        "    )\n",
        "    # Loading the weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 2) Read and preprocess the test file\n",
        "    if not os.path.exists(test_file):\n",
        "        print(f\"Test file '{test_file}' not found.\")\n",
        "        return\n",
        "\n",
        "    test_lines = read_lines_from_file(test_file)\n",
        "    # Prepare dataset\n",
        "    test_x, test_y = prepare_dataset(test_file, tokenizer, seq_len=32)\n",
        "    if test_x.size(0) == 0:\n",
        "        print(\"No data found in test file (after tokenization).\")\n",
        "        return\n",
        "\n",
        "    # 3) Generate tokens for each line\n",
        "    print(\"\\n--- Generation on each test line ---\")\n",
        "    for line in test_lines:\n",
        "        prompt = line.strip()\n",
        "        gen_text = generate_text(model, prompt, tokenizer, max_new_tokens=max_new_tokens)\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Generated: {gen_text}\")\n",
        "        print(\"----\")\n",
        "\n",
        "    # 4) Calculate perplexity on the entire test set ---\n",
        "    criterion = LabelSmoothingLoss(smoothing=0.1, vocab_size=len(tokenizer), ignore_index=pad_idx)\n",
        "\n",
        "    test_loss, test_ppl = 0.0, 0.0\n",
        "    test_count = 0\n",
        "\n",
        "    def get_test_batches(x_data, y_data, bsz):\n",
        "        for i in range(0, x_data.size(0), bsz):\n",
        "            yield x_data[i:i+bsz], y_data[i:i+bsz]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for bx, by in get_test_batches(test_x, test_y, batch_size):\n",
        "            bx, by = bx.to(device), by.to(device)\n",
        "            logits = model(bx)  # (batch, seq, vocab)\n",
        "            loss, ppl = calculate_loss_and_perplexity(logits, by, criterion, pad_idx=pad_idx)\n",
        "            test_loss += loss.item()\n",
        "            test_ppl += ppl\n",
        "            test_count += 1\n",
        "\n",
        "    test_loss /= test_count\n",
        "    test_ppl  /= test_count\n",
        "    print(f\"\\n--- Test Metrics ---\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Perplexity: {test_ppl:.2f}\\n\")\n",
        "\n",
        "def main():\n",
        "    train_file = \"/kaggle/input/shakespear/shakespear_train.txt\"\n",
        "    dev_file   = \"/kaggle/input/shakespear/shakespear_dev.txt\"\n",
        "\n",
        "    train_lines = read_lines_from_file(train_file)\n",
        "    dev_lines   = read_lines_from_file(dev_file)\n",
        "\n",
        "    tokenizer = CustomTokenizer()\n",
        "    tokenizer.build_vocab(train_lines + dev_lines)\n",
        "\n",
        "    model_save_path = \"/kaggle/working/transformer_shakespeare.pt\"\n",
        "    test_file = \"/kaggle/input/shakespear/shakespear_dev.txt\"\n",
        "    inference(model_save_path, test_file, tokenizer, pad_idx=tokenizer.token2idx[\"<PAD>\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IpQ2hPW8Wu8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}